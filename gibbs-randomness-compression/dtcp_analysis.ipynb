{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726b5d6c",
   "metadata": {},
   "source": [
    "# Dual Tomographic Compression (DTC): Analysis\n",
    "\n",
    "## Gibbs randomness-compression proposition: An eï¬ƒcient deep learning\n",
    "    Mehmet Suezen  \n",
    "    (c) 2025   \n",
    "    License  Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e855462",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2c_data =  dill.load(open('d2c_data.dill', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitute_prune_data = d2c_data['magnitute_prune_data']\n",
    "random_prune_data = d2c_data['random_prune_data']\n",
    "cs_prune_data = d2c_data['cs_prune_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35587cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrepeat = 1 \n",
    "levels = np.array(magnitute_prune_data[0][0])\n",
    "magnitute_prune_performance = np.array(\n",
    "    [magnitute_prune_data[i][1] for i in range(nrepeat)]\n",
    ")\n",
    "random_prune_performance = np.array([random_prune_data[i][1] for i in range(nrepeat)])\n",
    "cs_prune_performance = np.array([cs_prune_data[i][1] for i in range(nrepeat)])\n",
    "\n",
    "magnitute_prune = np.mean(magnitute_prune_performance, axis=0)\n",
    "magnitute_prune_se = np.std(magnitute_prune_performance, axis=0) / np.sqrt(nrepeat)\n",
    "random_prune = np.mean(random_prune_performance, axis=0)\n",
    "random_prune_se = np.std(random_prune_performance, axis=0) / np.sqrt(nrepeat)\n",
    "cs_prune = np.mean(cs_prune_performance, axis=0)\n",
    "cs_prune_se = np.std(cs_prune_performance, axis=0) / np.sqrt(nrepeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitute_prune_s = sm.nonparametric.lowess(exog=levels, endog=magnitute_prune, frac=0.3)\n",
    "random_prune_s = sm.nonparametric.lowess(exog=levels, endog=random_prune, frac=0.3)\n",
    "cs_prune_s = sm.nonparametric.lowess(exog=levels, endog=cs_prune, frac=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e954b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "matplotlib.use('agg')\n",
    "font = {\"family\": \"normal\", \"weight\": \"bold\", \"size\": 16}\n",
    "plt.rc(\"font\", **font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cdc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close()\n",
    "plt.plot(levels, magnitute_prune, \"o\", label=\"Magnitute Only Raw\")\n",
    "plt.plot(magnitute_prune_s[:, 0], magnitute_prune_s[:, 1], \"--\",  label=\"Magnitute Only\")\n",
    "plt.plot(levels, random_prune, \"x\", label=\"Random prune raw\")\n",
    "plt.plot(random_prune_s[:, 0], random_prune_s[:, 1], \"-\",  label=\"Random prune\")\n",
    "plt.plot(levels, cs_prune, \"*\", label=\"Magnitute with CS raw\")\n",
    "plt.plot(cs_prune_s[:, 0], cs_prune_s[:, 1], \"-.\",  label=\"Magnitute with CS\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(f\"Percent Neurons removed: \")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(f\" Iterated Dual Tomographic Compression \\n Advantage of Reconstruction \")\n",
    "plt.savefig(f\"dtc_accuracy.eps\",  format=\"eps\", dpi=1000, bbox_inches=\"tight\")  \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96394628",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(levels, magnitute_prune, \"o\", label=\"Magnitute Only Raw\")\n",
    "plt.plot(magnitute_prune_s[:, 0], magnitute_prune_s[:, 1], \"--\",  label=\"Magnitute Only\")\n",
    "plt.plot(levels, random_prune, \"x\", label=\"Random prune raw\")\n",
    "plt.plot(random_prune_s[:, 0], random_prune_s[:, 1], \"-\",  label=\"Random prune\")\n",
    "plt.plot(levels, cs_prune, \"*\", label=\"Magnitute with CS raw\")\n",
    "plt.plot(cs_prune_s[:, 0], cs_prune_s[:, 1], \"-.\",  label=\"Magnitute with CS\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim([0.8, 0.999])\n",
    "\n",
    "plt.xlabel(f\"Percent Neurons removed: \")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(f\" Iterated Dual Tomographic Compression \\n Advantage of Reconstruction \")\n",
    "plt.savefig(f\"dtc_accuracy_closeup.eps\",  format=\"eps\", dpi=1000, bbox_inches=\"tight\")  \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_performance = pd.DataFrame(\n",
    "    {\n",
    "        \"sparsity\": magnitute_prune_s[:, 0],\n",
    "        \"magnitute_prune\": magnitute_prune_s[:, 1],\n",
    "        \"random_prune\": random_prune_s[:, 1],\n",
    "        \"cs_prune\": cs_prune_s[:, 1],\n",
    "    }\n",
    ")\n",
    "df_performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c87fe",
   "metadata": {},
   "source": [
    "## Weight rays  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e98f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_prune_data = d2c_data['cs_prune_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evolution of total weight reconstructions.\n",
    "# \n",
    "# total reconstructed weights: \n",
    "# weight_rays \n",
    "#\n",
    "# Total weights : I/O hidden layers.\n",
    "#\n",
    "levels = cs_prune_data[0][0] # sparsity levels\n",
    "ll = len(levels)\n",
    "weight_rays_absmean = [] \n",
    "for j in range(1): # repeats\n",
    "     weight_rays = [cs_prune_data[j][2][i] for i in range(ll)]\n",
    "     weight_rays_absmean.append([np.mean(np.abs(ray))/len(ray) for ray in weight_rays])\n",
    "weight_rays_absmean = np.mean(weight_rays_absmean, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63215bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.step(levels, weight_rays_absmean)\n",
    "plt.xlabel(f\"Percent Neurons removed: \")\n",
    "plt.ylabel(\"Absolute Mean of Weight Ray \\n Scaled with size\")\n",
    "plt.title(f\" Weight Rays Absolute Mean \")\n",
    "plt.savefig(f\"dtc_weight_rays.eps\",  format=\"eps\", dpi=1000, bbox_inches=\"tight\")  \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b187ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Gibbs randomness-compression proposition \n",
    "# \n",
    "levels = cs_prune_data[0][0] # sparsity levels\n",
    "ll = len(levels)\n",
    "theta1s = cs_prune_data[0][5] # CS measurement vectors\n",
    "theta2s = cs_prune_data[0][6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in [5, 10, 15, 20]:\n",
    "    y, x = np.histogram(np.ndarray.flatten((theta1s[i])), bins=np.arange(-50.0,50.0,0.2))\n",
    "    u = y/np.sum(y)\n",
    "    plt.step(x[1:], u, label=f\"Sparsity level {levels[i]:.2f}\")\n",
    "plt.xlabel(f\"CS Measurement vector states \")\n",
    "plt.ylabel(\"Probability of occurance\")\n",
    "plt.title(f\" CS measurement occurance probabilities \")\n",
    "plt.legend()\n",
    "plt.savefig(f\"dtc_cs_measure_prob_y1.eps\", format=\"eps\", dpi=1000, bbox_inches=\"tight\")  \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gibbs_entropy = []\n",
    "for i in range(ll):\n",
    "    y, x = np.histogram(np.ndarray.flatten((theta1s[i])), bins=np.arange(-50.0,50.0,0.2))\n",
    "    u = y/np.sum(y)\n",
    "    gibbs_entropy.append(-1.0*np.sum([p * np.log2(p+1e-10) for p in u]))\n",
    "plt.plot(levels, gibbs_entropy, 'o')\n",
    "plt.xlabel(f\"Percent Neurons removed \")\n",
    "plt.ylabel(\"Gibbs Entropy (bits)\")\n",
    "plt.title(f\" Evolution of Entropy \\n of \\n Measurement vectors previous layer \")\n",
    "plt.savefig(f\"dtc_cs_gibbs_y1.eps\",  format=\"eps\", dpi=1000, bbox_inches=\"tight\")  \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aa2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation between random performance and Gibbs entropy\n",
    "np.corrcoef(gibbs_entropy, df_performance['cs_prune'])[0,1], np.corrcoef(gibbs_entropy, df_performance['random_prune'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49deedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in [5, 10, 15, 20]:\n",
    "    y, x = np.histogram(np.ndarray.flatten((theta2s[i])), bins=np.arange(-50.0,50.0,0.2))\n",
    "    u = y/np.sum(y)\n",
    "    plt.step(x[1:], u, label=f\"Sparsity level {levels[i]:.2f}\")\n",
    "plt.xlabel(f\"CS Measurement vector states \")\n",
    "plt.ylabel(\"Probability of occurance\")\n",
    "plt.title(f\" CS measurement occurance probabilities \")\n",
    "plt.legend()\n",
    "plt.savefig(f\"dtc_cs_measure_prob_y2.eps\",  format=\"eps\", dpi=1000, bbox_inches=\"tight\")  \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gibbs_entropy = []\n",
    "for i in range(ll):\n",
    "    y, x = np.histogram(np.ndarray.flatten((theta2s[i])), bins=np.arange(-50.0,50.0,0.2))\n",
    "    u = y/np.sum(y)\n",
    "    gibbs_entropy.append(-1.0*np.sum([p * np.log(p+1e-10) for p in u]))\n",
    "plt.plot(levels, gibbs_entropy, 'o')\n",
    "plt.xlabel(f\"Percent Neurons removed \")\n",
    "plt.ylabel(\"Gibbs Entropy (bits)\")\n",
    "plt.title(f\" Evolution of Entropy \\n of \\n Measurement vectors next layer \")\n",
    "plt.savefig(f\"dtc_cs_gibbs_y2.eps\",  format=\"eps\", dpi=1000, bbox_inches=\"tight\")  \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(gibbs_entropy, df_performance['cs_prune'])[0,1], np.corrcoef(gibbs_entropy, df_performance['random_prune'])[0,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twofortyice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
