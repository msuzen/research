{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Tomographic Compression (DTC): Generate Data\n",
    "\n",
    "## Gibbs randomness-compression proposition: An eï¬ƒcient deep learning\n",
    "\n",
    "    Mehmet Suezen  \n",
    "    (c) 2025   \n",
    "    License  Apache 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versions \n",
    "\n",
    "```\n",
    "# \n",
    "# Packages used Python 2.10.13\n",
    "#\n",
    "torch==2.6.0\n",
    "torchvision==0.21.0\n",
    "matplotlib==3.10.1\n",
    "numpy==1.26.2\n",
    "cvxpy==1.6.4\n",
    "scipy==1.15.2\n",
    "dill==0.4.0\n",
    "statsmodels==0.14.4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import packages.\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import matplotlib\n",
    "import numpy\n",
    "import cvxpy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('2.6.0', '0.21.0', '3.10.1', '1.26.2', '1.6.4', '1.15.2')\n",
    "torch.__version__, torchvision.__version__, matplotlib.__version__, numpy.__version__, cvxpy.__version__, scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]  #  mu/sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\n",
    "\n",
    "g_cpu = torch.Generator()\n",
    "g_cpu.manual_seed(42424242)\n",
    "        \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=False, generator=g_cpu\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, generator=g_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x, y in train_loader:\n",
    "    i = i + 1\n",
    "i\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseM(nn.Module): # Fixed seed\n",
    "    def __init__(self, nsize, initial_type=\"xavier\", seed=42424242): # CPU\n",
    "        super(BaseM, self).__init__()\n",
    "        self.input = nn.Flatten()\n",
    "        self.bnorm = nn.BatchNorm1d(28 * 28)\n",
    "        self.linear1 = nn.Linear(28 * 28, nsize, bias=False)\n",
    "        self.linear2 = nn.Linear(nsize, 10, bias=False)\n",
    "        self.softmax_out = nn.LogSoftmax(dim=0)\n",
    "\n",
    "        g_cpu = torch.Generator()\n",
    "        g_cpu.manual_seed(seed)\n",
    "        if initial_type == \"xavier\":\n",
    "            _ = torch.nn.init.xavier_normal_(self.linear1.weight, generator=g_cpu)\n",
    "            _ = torch.nn.init.xavier_normal_(self.linear2.weight, generator=g_cpu)\n",
    "\n",
    "        if initial_type == \"he\":\n",
    "            _ = torch.nn.init.kaiming_normal_(\n",
    "                self.linear1.weight, mode=\"fan_in\", nonlinearity=\"relu\", generator=g_cpu\n",
    "            )\n",
    "            _ = torch.nn.init.kaiming_normal_(\n",
    "                self.linear1.weight, mode=\"fan_in\", nonlinearity=\"relu\", generator=g_cpu\n",
    "            )\n",
    "\n",
    "    def forward(self, image_mnist):\n",
    "        input_flat = self.input(image_mnist)\n",
    "        linear1 = self.linear1(input_flat)\n",
    "        linear1_relu = F.relu(linear1)\n",
    "        linear2 = self.linear2(linear1_relu)\n",
    "        output = self.softmax_out(linear2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels).item() / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy(outputs, labels)\n",
    "    # print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_acc / len(test_loader):.4f}')\n",
    "    return test_acc / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, device, test_loader, criterion, nbatch=100):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    nb = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy(outputs, labels)\n",
    "            nb = nb + 1\n",
    "            if nb > nbatch:\n",
    "                break\n",
    "    # print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_acc / len(test_loader):.4f}')\n",
    "    return test_acc / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model_dyn, number_of_batches=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_dyn.parameters(), lr=learning_rate)\n",
    "    test_acc = test(model_dyn, device, test_loader, criterion)\n",
    "    i = 0\n",
    "    for _, (inputs, labels) in itertools.cycle(enumerate(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_dyn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i = i + 1\n",
    "        if i == number_of_batches:\n",
    "            test_acc = test(model_dyn, device, test_loader, criterion)\n",
    "            return model_dyn, test_acc\n",
    "    return model_dyn, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now inverse\n",
    "def relative_error(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the relative error between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    x (numpy.ndarray): The first vector.\n",
    "    y (numpy.ndarray): The second vector.\n",
    "\n",
    "    Returns/:\n",
    "    float: The relative error between the two vectors.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x - y) / np.linalg.norm(y)\n",
    "\n",
    "\n",
    "def mape_vectorized_v2(a, b):\n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b) / a)[mask].mean()\n",
    "\n",
    "\n",
    "def generate_dct_matrix(n, dct_type=1):\n",
    "    return scipy.fftpack.dct(np.eye(n), norm=\"ortho\", axis=0, type=dct_type)\n",
    "\n",
    "\n",
    "def cs_inverse(x_org, sparsity_level=0.05, Lambda=1.5, seed=42424242):\n",
    "    np.random.seed(seed=seed)\n",
    "    m = len(x_org)\n",
    "    # Up to Measurements  M > K LOG (N/K) , K sparse components\n",
    "    n = int(m * (1.0 - sparsity_level))  #\n",
    "    #\n",
    "    Phi = np.random.normal(size=[n, m])  # Gaussian random (measurement matrix) ;\n",
    "    # Theta = Phi  * Psi  (Psi   sparsification matrix)\n",
    "    # Recall https://users.soe.ucsc.edu/~afletcher/EE293/Week1Readings/Papers_Week1_and_Week2/Baraniuk_SPMag2007.pdf\n",
    "    Psi = generate_dct_matrix(m)\n",
    "    Theta = np.matmul(Phi, Psi)\n",
    "    # Define and solve the CVXPY problem.\n",
    "    weights_sparse = cp.Variable(m)\n",
    "    y_measurement = np.matmul(Phi, x_org)  # make a measurement\n",
    "    cost = cp.sum_squares(\n",
    "        Phi @ Psi @ weights_sparse - y_measurement\n",
    "    ) + Lambda * cp.norm1(weights_sparse)\n",
    "    prob = cp.Problem(cp.Minimize(cost))\n",
    "    _ = prob.solve(solver=\"SCS\") #  SCS,  \n",
    "    weights_sparse_value = weights_sparse.value\n",
    "    x_reconstructed = np.matmul(Psi, weights_sparse_value)\n",
    "    mape = mape_vectorized_v2(x_reconstructed, x_org)\n",
    "    return weights_sparse_value, x_reconstructed, mape, Theta, y_measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(\n",
    "    model_org, sparsity_level=0.05, cs=False, random_prune=False, Lambda=1.5\n",
    "):  #  defaults to magnitute prune\n",
    "    l1_weights = model_org.state_dict()[\"linear1.weight\"]\n",
    "    l2_weights = model_org.state_dict()[\"linear2.weight\"]\n",
    "    A1 = np.array(np.array(l1_weights).sum(axis=1))\n",
    "    A2 = np.array(np.array(l2_weights).sum(axis=0))\n",
    "    w12 = A1 + A2\n",
    "    mape1 = 0.0\n",
    "    mape2 = 0.0\n",
    "    Theta1 = -1  # if Not CS\n",
    "    Theta2 = -1\n",
    "    y1 = -1 \n",
    "    y2 = -1\n",
    "    if cs:\n",
    "        weights_sparse_value1, _, mape1, Theta1, y1 = cs_inverse(\n",
    "            A1, sparsity_level=sparsity_level, Lambda=Lambda\n",
    "        )\n",
    "        weights_sparse_value2, _, mape2, Theta2, y2 = cs_inverse(\n",
    "            A2, sparsity_level=sparsity_level, Lambda=Lambda\n",
    "        )\n",
    "        w12 = weights_sparse_value1 + weights_sparse_value2\n",
    "    # find units to  magnitute clip (on CS weights or learned weights)\n",
    "    keep_indices = np.where(np.abs(w12) < np.quantile(np.abs(w12), sparsity_level))[0]\n",
    "    keep_id_length = len(keep_indices) \n",
    "    if random_prune:\n",
    "       np.random.seed(42424242)\n",
    "       keep_indices = np.random.choice(len(w12), keep_id_length, replace=False)\n",
    "    sparse_model = BaseM(nsize=len(keep_indices))\n",
    "    model_weights = sparse_model.state_dict()\n",
    "    model_weights[\"linear1.weight\"] = l1_weights[keep_indices, :]\n",
    "    model_weights[\"linear2.weight\"] = l2_weights[:, keep_indices]\n",
    "    sparse_model.load_state_dict(model_weights)\n",
    "    return sparse_model, mape1, mape2, w12, Theta1, Theta2, y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# CS Inverse: 90% sparsity \n",
    "# Iterative Sparsification and training \n",
    "#\n",
    "def iterative_train_prune(\n",
    "    model,\n",
    "    is_cs=False,\n",
    "    random_prune=False,\n",
    "    number_of_batches=100,\n",
    "    stop_size=10,\n",
    "    sparsity_level=0.99,\n",
    "    Lambda=1.5,\n",
    "):\n",
    "    nsize_reduced = 512\n",
    "    nsize = 512\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_errors = []\n",
    "    sparsity_levels = []\n",
    "    total_weights = []\n",
    "    theta1 = []\n",
    "    theta2 = []\n",
    "    ymeasure1 = []\n",
    "    ymeasure2 = []\n",
    "    i = 0\n",
    "    while nsize_reduced > stop_size:\n",
    "        model, acc = model_train(model, number_of_batches=number_of_batches)\n",
    "        model, mape1, mape2, w12, t1, t2, y1, y2 = prune_model(\n",
    "            model,\n",
    "            sparsity_level=sparsity_level,\n",
    "            cs=is_cs,\n",
    "            random_prune=random_prune,\n",
    "            Lambda=Lambda,\n",
    "        )\n",
    "        nsize_reduced = model.state_dict()[\"linear1.weight\"].shape[0]\n",
    "        actual_sparsity_level = 1.0 - nsize_reduced / nsize\n",
    "        # print(f\"iter-batch-group {i} nsize_reduced {nsize_reduced}\")\n",
    "        # print(\n",
    "        #     f\"      mape1={mape1} mape2={mape2} acc={acc} actual_sparsity_level={actual_sparsity_level}\"\n",
    "        # )\n",
    "\n",
    "        # print(\"  \")\n",
    "        # print(\"  Test on sparse\")\n",
    "        # print(\"  \")\n",
    "        test_error = test(model, device, test_loader, criterion)\n",
    "        test_errors.append(test_error)\n",
    "        sparsity_levels.append(actual_sparsity_level)\n",
    "        total_weights.append(w12)\n",
    "        theta1.append(t1)\n",
    "        theta2.append(t2)\n",
    "        ymeasure1.append(y1)\n",
    "        ymeasure2.append(y2)\n",
    "        print(i, test_error, model.state_dict()[\"linear1.weight\"].shape[0], actual_sparsity_level)\n",
    "        i = i + 1\n",
    "    return sparsity_levels, test_errors, total_weights, theta1, theta2, ymeasure1, ymeasure2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model\n",
    "nsize = 512\n",
    "batch_size = batch_size\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model0 = BaseM(nsize=nsize).to(\"cpu\")\n",
    "model, acc = model_train(model0, number_of_batches=1)  #  \n",
    "test_error_full = test(model, device, test_loader, criterion)\n",
    "print(\"full model test error=\", test_error_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds are set for initialisation and batches\n",
    "# So results are deterministic on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitute prune (default) : \n",
    "nrepeat = 1 \n",
    "magnitute_prune_data = []\n",
    "model = BaseM(nsize=nsize).to(\"cpu\") # fixed seed\n",
    "for _ in range(nrepeat):\n",
    "    sparsity_levels_m, test_errors_m, _, _, _, _, _ = iterative_train_prune(\n",
    "        model, sparsity_level=0.9, number_of_batches=200\n",
    "    )\n",
    "    magnitute_prune_data.append(\n",
    "        (\n",
    "            sparsity_levels_m,\n",
    "            test_errors_m,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prune\n",
    "nrepeat = 1\n",
    "random_prune_data = []\n",
    "model = BaseM(nsize=nsize).to(\"cpu\") # fixed seed\n",
    "for _ in range(nrepeat):\n",
    "    sparsity_levels_m, test_errors_m, _, _, _, _, _ = iterative_train_prune(\n",
    "        model, random_prune=True, sparsity_level=0.9, number_of_batches=200\n",
    "    )\n",
    "    random_prune_data.append(\n",
    "        (\n",
    "            sparsity_levels_m,\n",
    "            test_errors_m,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS prune\n",
    "nrepeat = 1 \n",
    "cs_prune_data = []\n",
    "model = BaseM(nsize=nsize).to(\"cpu\") # fixed seed\n",
    "for _ in range(nrepeat):\n",
    "    sparsity_levels_m, test_errors_m, total_weights_cs, theta1, theta2, y1, y2 = iterative_train_prune(\n",
    "            model, \n",
    "            is_cs=True,\n",
    "            Lambda=0.3,\n",
    "            sparsity_level=0.9,\n",
    "            number_of_batches=200,\n",
    "        )\n",
    "    cs_prune_data.append(\n",
    "        (sparsity_levels_m, test_errors_m, total_weights_cs, theta1, theta2, y1, y2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2c_data = {\n",
    "    \"test_error_full\": test_error_full,\n",
    "    \"magnitute_prune_data\": magnitute_prune_data,\n",
    "    \"random_prune_data\": random_prune_data,\n",
    "    \"cs_prune_data\":cs_prune_data\n",
    "}\n",
    "\n",
    "import dill\n",
    "dill.dump(d2c_data, open(\"d2c_data.dill\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.E.D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twofortyice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
